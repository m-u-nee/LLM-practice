{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We implement a simple version of GPT 2 using the transformer architecture, following attention is all you need paper\n",
    "# In our case, we build a character level language model, which will be trained on tinyshakespeare dataset\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data\n",
    "with open('tinyshakespeare.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of data: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters:  65\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars) \n",
    "print(\"number of unique characters: \", vocab_size)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "encode = lambda x: [stoi[ch] for ch in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "print(encode('hello'))\n",
    "print(decode(encode('hello')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in to train anv validation\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data= data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# # visualize a batch\n",
    "xb, yb = get_batch('train')\n",
    "# print('inputs:')\n",
    "# print(xb.shape, xb.dtype)\n",
    "# print(xb)\n",
    "# print('targets:')\n",
    "# print(yb.shape, yb.dtype)\n",
    "# print(yb)\n",
    "\n",
    "# print(\"-------\")\n",
    "# for b in range(batch_size):\n",
    "#     for t in range(block_size):\n",
    "#         context = xb[b, :t+1]\n",
    "#         target = yb[b, t]\n",
    "#         print(f'When input is {context.tolist()} target is {target.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensors\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]  # Becomes (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "print(out[0].shape)\n",
    "print(out[1])\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "\n",
    "print(decode(m.generate(idx, 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop \n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 2.571192741394043\n",
      "step: 100, loss: 2.4889583587646484\n",
      "step: 200, loss: 2.544487714767456\n",
      "step: 300, loss: 2.5225296020507812\n",
      "step: 400, loss: 2.5682666301727295\n",
      "step: 500, loss: 2.3110439777374268\n",
      "step: 600, loss: 2.4499902725219727\n",
      "step: 700, loss: 2.5348362922668457\n",
      "step: 800, loss: 2.47645902633667\n",
      "step: 900, loss: 2.4677135944366455\n",
      "step: 1000, loss: 2.4076132774353027\n",
      "step: 1100, loss: 2.576707124710083\n",
      "step: 1200, loss: 2.429227590560913\n",
      "step: 1300, loss: 2.5243372917175293\n",
      "step: 1400, loss: 2.399763345718384\n",
      "step: 1500, loss: 2.4520907402038574\n",
      "step: 1600, loss: 2.466071844100952\n",
      "step: 1700, loss: 2.38797664642334\n",
      "step: 1800, loss: 2.599693536758423\n",
      "step: 1900, loss: 2.4532437324523926\n",
      "step: 2000, loss: 2.4377825260162354\n",
      "step: 2100, loss: 2.4375531673431396\n",
      "step: 2200, loss: 2.3875503540039062\n",
      "step: 2300, loss: 2.6257407665252686\n",
      "step: 2400, loss: 2.3347315788269043\n",
      "step: 2500, loss: 2.494647741317749\n",
      "step: 2600, loss: 2.5356574058532715\n",
      "step: 2700, loss: 2.46797513961792\n",
      "step: 2800, loss: 2.4057836532592773\n",
      "step: 2900, loss: 2.45854115486145\n",
      "step: 3000, loss: 2.4470880031585693\n",
      "step: 3100, loss: 2.5215108394622803\n",
      "step: 3200, loss: 2.443779945373535\n",
      "step: 3300, loss: 2.4198837280273438\n",
      "step: 3400, loss: 2.361316204071045\n",
      "step: 3500, loss: 2.3830151557922363\n",
      "step: 3600, loss: 2.506589412689209\n",
      "step: 3700, loss: 2.5583643913269043\n",
      "step: 3800, loss: 2.4960241317749023\n",
      "step: 3900, loss: 2.591376781463623\n",
      "step: 4000, loss: 2.453509569168091\n",
      "step: 4100, loss: 2.50333571434021\n",
      "step: 4200, loss: 2.468052864074707\n",
      "step: 4300, loss: 2.6402740478515625\n",
      "step: 4400, loss: 2.419400215148926\n",
      "step: 4500, loss: 2.4801931381225586\n",
      "step: 4600, loss: 2.2806074619293213\n",
      "step: 4700, loss: 2.6247169971466064\n",
      "step: 4800, loss: 2.312093734741211\n",
      "step: 4900, loss: 2.4751157760620117\n",
      "step: 5000, loss: 2.3857340812683105\n",
      "step: 5100, loss: 2.4573400020599365\n",
      "step: 5200, loss: 2.454707622528076\n",
      "step: 5300, loss: 2.6132826805114746\n",
      "step: 5400, loss: 2.4800548553466797\n",
      "step: 5500, loss: 2.3708722591400146\n",
      "step: 5600, loss: 2.252281665802002\n",
      "step: 5700, loss: 2.315114974975586\n",
      "step: 5800, loss: 2.4472193717956543\n",
      "step: 5900, loss: 2.4677653312683105\n",
      "step: 6000, loss: 2.4524829387664795\n",
      "step: 6100, loss: 2.515334129333496\n",
      "step: 6200, loss: 2.3762354850769043\n",
      "step: 6300, loss: 2.5024211406707764\n",
      "step: 6400, loss: 2.482365369796753\n",
      "step: 6500, loss: 2.4465863704681396\n",
      "step: 6600, loss: 2.492169141769409\n",
      "step: 6700, loss: 2.594869375228882\n",
      "step: 6800, loss: 2.5197978019714355\n",
      "step: 6900, loss: 2.387796640396118\n",
      "step: 7000, loss: 2.327974557876587\n",
      "step: 7100, loss: 2.490089178085327\n",
      "step: 7200, loss: 2.5130631923675537\n",
      "step: 7300, loss: 2.5322680473327637\n",
      "step: 7400, loss: 2.409907579421997\n",
      "step: 7500, loss: 2.537959098815918\n",
      "step: 7600, loss: 2.5346126556396484\n",
      "step: 7700, loss: 2.631098747253418\n",
      "step: 7800, loss: 2.4485867023468018\n",
      "step: 7900, loss: 2.44865345954895\n",
      "step: 8000, loss: 2.34757924079895\n",
      "step: 8100, loss: 2.3756790161132812\n",
      "step: 8200, loss: 2.4041507244110107\n",
      "step: 8300, loss: 2.424164295196533\n",
      "step: 8400, loss: 2.5234780311584473\n",
      "step: 8500, loss: 2.4142377376556396\n",
      "step: 8600, loss: 2.5034708976745605\n",
      "step: 8700, loss: 2.55928373336792\n",
      "step: 8800, loss: 2.465740203857422\n",
      "step: 8900, loss: 2.388139247894287\n",
      "step: 9000, loss: 2.4133880138397217\n",
      "step: 9100, loss: 2.3352363109588623\n",
      "step: 9200, loss: 2.386059522628784\n",
      "step: 9300, loss: 2.3517706394195557\n",
      "step: 9400, loss: 2.4521450996398926\n",
      "step: 9500, loss: 2.563115358352661\n",
      "step: 9600, loss: 2.4383444786071777\n",
      "step: 9700, loss: 2.368190050125122\n",
      "step: 9800, loss: 2.38187313079834\n",
      "step: 9900, loss: 2.4677681922912598\n",
      "step: 10000, loss: 2.421082019805908\n",
      "step: 10100, loss: 2.5706958770751953\n",
      "step: 10200, loss: 2.5765492916107178\n",
      "step: 10300, loss: 2.4120423793792725\n",
      "step: 10400, loss: 2.5040433406829834\n",
      "step: 10500, loss: 2.358886241912842\n",
      "step: 10600, loss: 2.3389742374420166\n",
      "step: 10700, loss: 2.4885218143463135\n",
      "step: 10800, loss: 2.5030412673950195\n",
      "step: 10900, loss: 2.503455638885498\n",
      "step: 11000, loss: 2.4679620265960693\n",
      "step: 11100, loss: 2.544670343399048\n",
      "step: 11200, loss: 2.443784475326538\n",
      "step: 11300, loss: 2.381159782409668\n",
      "step: 11400, loss: 2.463747978210449\n",
      "step: 11500, loss: 2.491126537322998\n",
      "step: 11600, loss: 2.401386260986328\n",
      "step: 11700, loss: 2.4818098545074463\n",
      "step: 11800, loss: 2.4766790866851807\n",
      "step: 11900, loss: 2.338893413543701\n",
      "step: 12000, loss: 2.4371254444122314\n",
      "step: 12100, loss: 2.4703259468078613\n",
      "step: 12200, loss: 2.406477928161621\n",
      "step: 12300, loss: 2.5425071716308594\n",
      "step: 12400, loss: 2.4007568359375\n",
      "step: 12500, loss: 2.367983102798462\n",
      "step: 12600, loss: 2.424131155014038\n",
      "step: 12700, loss: 2.412445306777954\n",
      "step: 12800, loss: 2.5772924423217773\n",
      "step: 12900, loss: 2.4101486206054688\n",
      "step: 13000, loss: 2.3554511070251465\n",
      "step: 13100, loss: 2.50724196434021\n",
      "step: 13200, loss: 2.3493833541870117\n",
      "step: 13300, loss: 2.461214542388916\n",
      "step: 13400, loss: 2.4147071838378906\n",
      "step: 13500, loss: 2.368736505508423\n",
      "step: 13600, loss: 2.384889602661133\n",
      "step: 13700, loss: 2.364269733428955\n",
      "step: 13800, loss: 2.4711382389068604\n",
      "step: 13900, loss: 2.4843716621398926\n",
      "step: 14000, loss: 2.4077203273773193\n",
      "step: 14100, loss: 2.529679775238037\n",
      "step: 14200, loss: 2.5229732990264893\n",
      "step: 14300, loss: 2.374103307723999\n",
      "step: 14400, loss: 2.3799829483032227\n",
      "step: 14500, loss: 2.457968235015869\n",
      "step: 14600, loss: 2.439135789871216\n",
      "step: 14700, loss: 2.5029633045196533\n",
      "step: 14800, loss: 2.515537738800049\n",
      "step: 14900, loss: 2.4668517112731934\n",
      "step: 15000, loss: 2.342461585998535\n",
      "step: 15100, loss: 2.4911692142486572\n",
      "step: 15200, loss: 2.515868663787842\n",
      "step: 15300, loss: 2.469615936279297\n",
      "step: 15400, loss: 2.5849380493164062\n",
      "step: 15500, loss: 2.326748847961426\n",
      "step: 15600, loss: 2.3509738445281982\n",
      "step: 15700, loss: 2.474317789077759\n",
      "step: 15800, loss: 2.3142142295837402\n",
      "step: 15900, loss: 2.576817750930786\n",
      "step: 16000, loss: 2.383704900741577\n",
      "step: 16100, loss: 2.393509864807129\n",
      "step: 16200, loss: 2.3783507347106934\n",
      "step: 16300, loss: 2.505993127822876\n",
      "step: 16400, loss: 2.4011192321777344\n",
      "step: 16500, loss: 2.454514741897583\n",
      "step: 16600, loss: 2.479740858078003\n",
      "step: 16700, loss: 2.3792524337768555\n",
      "step: 16800, loss: 2.3152921199798584\n",
      "step: 16900, loss: 2.533823251724243\n",
      "step: 17000, loss: 2.3697614669799805\n",
      "step: 17100, loss: 2.480426788330078\n",
      "step: 17200, loss: 2.4065282344818115\n",
      "step: 17300, loss: 2.4857089519500732\n",
      "step: 17400, loss: 2.471035957336426\n",
      "step: 17500, loss: 2.4400198459625244\n",
      "step: 17600, loss: 2.4235329627990723\n",
      "step: 17700, loss: 2.474184274673462\n",
      "step: 17800, loss: 2.48807954788208\n",
      "step: 17900, loss: 2.4986817836761475\n",
      "step: 18000, loss: 2.4317896366119385\n",
      "step: 18100, loss: 2.5726301670074463\n",
      "step: 18200, loss: 2.574610948562622\n",
      "step: 18300, loss: 2.468787431716919\n",
      "step: 18400, loss: 2.527163505554199\n",
      "step: 18500, loss: 2.481501340866089\n",
      "step: 18600, loss: 2.3786263465881348\n",
      "step: 18700, loss: 2.363957405090332\n",
      "step: 18800, loss: 2.544667959213257\n",
      "step: 18900, loss: 2.453514814376831\n",
      "step: 19000, loss: 2.4810123443603516\n",
      "step: 19100, loss: 2.3638112545013428\n",
      "step: 19200, loss: 2.5093417167663574\n",
      "step: 19300, loss: 2.3138294219970703\n",
      "step: 19400, loss: 2.419196844100952\n",
      "step: 19500, loss: 2.3857200145721436\n",
      "step: 19600, loss: 2.5532314777374268\n",
      "step: 19700, loss: 2.4456100463867188\n",
      "step: 19800, loss: 2.459707736968994\n",
      "step: 19900, loss: 2.5103330612182617\n",
      "step: 20000, loss: 2.518850803375244\n",
      "step: 20100, loss: 2.6218791007995605\n",
      "step: 20200, loss: 2.3733878135681152\n",
      "step: 20300, loss: 2.38438081741333\n",
      "step: 20400, loss: 2.497514247894287\n",
      "step: 20500, loss: 2.3744442462921143\n",
      "step: 20600, loss: 2.6287479400634766\n",
      "step: 20700, loss: 2.3775129318237305\n",
      "step: 20800, loss: 2.5397768020629883\n",
      "step: 20900, loss: 2.481074333190918\n",
      "step: 21000, loss: 2.5434517860412598\n",
      "step: 21100, loss: 2.433074712753296\n",
      "step: 21200, loss: 2.4218311309814453\n",
      "step: 21300, loss: 2.4970602989196777\n",
      "step: 21400, loss: 2.4584407806396484\n",
      "step: 21500, loss: 2.533114433288574\n",
      "step: 21600, loss: 2.418036460876465\n",
      "step: 21700, loss: 2.464279890060425\n",
      "step: 21800, loss: 2.541168212890625\n",
      "step: 21900, loss: 2.4433505535125732\n",
      "step: 22000, loss: 2.3853092193603516\n",
      "step: 22100, loss: 2.5299270153045654\n",
      "step: 22200, loss: 2.5538294315338135\n",
      "step: 22300, loss: 2.3982949256896973\n",
      "step: 22400, loss: 2.384094476699829\n",
      "step: 22500, loss: 2.4300878047943115\n",
      "step: 22600, loss: 2.408116579055786\n",
      "step: 22700, loss: 2.4665634632110596\n",
      "step: 22800, loss: 2.4781064987182617\n",
      "step: 22900, loss: 2.5861706733703613\n",
      "step: 23000, loss: 2.5286972522735596\n",
      "step: 23100, loss: 2.3891096115112305\n",
      "step: 23200, loss: 2.4394867420196533\n",
      "step: 23300, loss: 2.48921799659729\n",
      "step: 23400, loss: 2.588167905807495\n",
      "step: 23500, loss: 2.541409969329834\n",
      "step: 23600, loss: 2.4410438537597656\n",
      "step: 23700, loss: 2.4891486167907715\n",
      "step: 23800, loss: 2.3956234455108643\n",
      "step: 23900, loss: 2.372668981552124\n",
      "step: 24000, loss: 2.4228515625\n",
      "step: 24100, loss: 2.462841510772705\n",
      "step: 24200, loss: 2.4173507690429688\n",
      "step: 24300, loss: 2.30489444732666\n",
      "step: 24400, loss: 2.3450405597686768\n",
      "step: 24500, loss: 2.5557878017425537\n",
      "step: 24600, loss: 2.274465322494507\n",
      "step: 24700, loss: 2.4794528484344482\n",
      "step: 24800, loss: 2.527031898498535\n",
      "step: 24900, loss: 2.446864366531372\n",
      "step: 25000, loss: 2.4697303771972656\n",
      "step: 25100, loss: 2.431328535079956\n",
      "step: 25200, loss: 2.5240402221679688\n",
      "step: 25300, loss: 2.4337713718414307\n",
      "step: 25400, loss: 2.368009567260742\n",
      "step: 25500, loss: 2.301701068878174\n",
      "step: 25600, loss: 2.3954265117645264\n",
      "step: 25700, loss: 2.47257137298584\n",
      "step: 25800, loss: 2.428328275680542\n",
      "step: 25900, loss: 2.5058815479278564\n",
      "step: 26000, loss: 2.3527252674102783\n",
      "step: 26100, loss: 2.484534740447998\n",
      "step: 26200, loss: 2.4310431480407715\n",
      "step: 26300, loss: 2.406630277633667\n",
      "step: 26400, loss: 2.3313286304473877\n",
      "step: 26500, loss: 2.4601969718933105\n",
      "step: 26600, loss: 2.362569570541382\n",
      "step: 26700, loss: 2.2864911556243896\n",
      "step: 26800, loss: 2.5339386463165283\n",
      "step: 26900, loss: 2.407174587249756\n",
      "step: 27000, loss: 2.5604002475738525\n",
      "step: 27100, loss: 2.481440544128418\n",
      "step: 27200, loss: 2.3521311283111572\n",
      "step: 27300, loss: 2.361323595046997\n",
      "step: 27400, loss: 2.551257848739624\n",
      "step: 27500, loss: 2.4553210735321045\n",
      "step: 27600, loss: 2.4036760330200195\n",
      "step: 27700, loss: 2.475118398666382\n",
      "step: 27800, loss: 2.2400708198547363\n",
      "step: 27900, loss: 2.375697612762451\n",
      "step: 28000, loss: 2.4360387325286865\n",
      "step: 28100, loss: 2.537661552429199\n",
      "step: 28200, loss: 2.3772170543670654\n",
      "step: 28300, loss: 2.445970296859741\n",
      "step: 28400, loss: 2.3401386737823486\n",
      "step: 28500, loss: 2.422642469406128\n",
      "step: 28600, loss: 2.4163079261779785\n",
      "step: 28700, loss: 2.5581679344177246\n",
      "step: 28800, loss: 2.555238723754883\n",
      "step: 28900, loss: 2.4543235301971436\n",
      "step: 29000, loss: 2.399965524673462\n",
      "step: 29100, loss: 2.4599015712738037\n",
      "step: 29200, loss: 2.466768741607666\n",
      "step: 29300, loss: 2.4239094257354736\n",
      "step: 29400, loss: 2.4993200302124023\n",
      "step: 29500, loss: 2.2851786613464355\n",
      "step: 29600, loss: 2.6147923469543457\n",
      "step: 29700, loss: 2.46512508392334\n",
      "step: 29800, loss: 2.428868532180786\n",
      "step: 29900, loss: 2.5175533294677734\n",
      "step: 30000, loss: 2.348202705383301\n",
      "step: 30100, loss: 2.2343945503234863\n",
      "step: 30200, loss: 2.4094622135162354\n",
      "step: 30300, loss: 2.3979556560516357\n",
      "step: 30400, loss: 2.53157639503479\n",
      "step: 30500, loss: 2.4477736949920654\n",
      "step: 30600, loss: 2.5030341148376465\n",
      "step: 30700, loss: 2.4618000984191895\n",
      "step: 30800, loss: 2.3723597526550293\n",
      "step: 30900, loss: 2.425746440887451\n",
      "step: 31000, loss: 2.393014430999756\n",
      "step: 31100, loss: 2.402050495147705\n",
      "step: 31200, loss: 2.5240063667297363\n",
      "step: 31300, loss: 2.4556024074554443\n",
      "step: 31400, loss: 2.3598570823669434\n",
      "step: 31500, loss: 2.483684778213501\n",
      "step: 31600, loss: 2.457535743713379\n",
      "step: 31700, loss: 2.3511300086975098\n",
      "step: 31800, loss: 2.3849847316741943\n",
      "step: 31900, loss: 2.386639356613159\n",
      "step: 32000, loss: 2.462923049926758\n",
      "step: 32100, loss: 2.3735992908477783\n",
      "step: 32200, loss: 2.408858299255371\n",
      "step: 32300, loss: 2.602360486984253\n",
      "step: 32400, loss: 2.536132574081421\n",
      "step: 32500, loss: 2.3867814540863037\n",
      "step: 32600, loss: 2.431602716445923\n",
      "step: 32700, loss: 2.378850221633911\n",
      "step: 32800, loss: 2.4695658683776855\n",
      "step: 32900, loss: 2.5063326358795166\n",
      "step: 33000, loss: 2.485095739364624\n",
      "step: 33100, loss: 2.3974876403808594\n",
      "step: 33200, loss: 2.3922946453094482\n",
      "step: 33300, loss: 2.425694465637207\n",
      "step: 33400, loss: 2.489875555038452\n",
      "step: 33500, loss: 2.4638795852661133\n",
      "step: 33600, loss: 2.4393351078033447\n",
      "step: 33700, loss: 2.4270131587982178\n",
      "step: 33800, loss: 2.538504123687744\n",
      "step: 33900, loss: 2.551231622695923\n",
      "step: 34000, loss: 2.237525463104248\n",
      "step: 34100, loss: 2.425156354904175\n",
      "step: 34200, loss: 2.4357893466949463\n",
      "step: 34300, loss: 2.409486770629883\n",
      "step: 34400, loss: 2.3217132091522217\n",
      "step: 34500, loss: 2.3864943981170654\n",
      "step: 34600, loss: 2.5012259483337402\n",
      "step: 34700, loss: 2.3943445682525635\n",
      "step: 34800, loss: 2.449613332748413\n",
      "step: 34900, loss: 2.4557623863220215\n",
      "step: 35000, loss: 2.378272294998169\n",
      "step: 35100, loss: 2.4548697471618652\n",
      "step: 35200, loss: 2.424981117248535\n",
      "step: 35300, loss: 2.466032028198242\n",
      "step: 35400, loss: 2.454395055770874\n",
      "step: 35500, loss: 2.4065499305725098\n",
      "step: 35600, loss: 2.5479092597961426\n",
      "step: 35700, loss: 2.552001953125\n",
      "step: 35800, loss: 2.4060397148132324\n",
      "step: 35900, loss: 2.3686819076538086\n",
      "step: 36000, loss: 2.4778549671173096\n",
      "step: 36100, loss: 2.4566924571990967\n",
      "step: 36200, loss: 2.4686832427978516\n",
      "step: 36300, loss: 2.3308424949645996\n",
      "step: 36400, loss: 2.450671672821045\n",
      "step: 36500, loss: 2.5464441776275635\n",
      "step: 36600, loss: 2.435884952545166\n",
      "step: 36700, loss: 2.547527551651001\n",
      "step: 36800, loss: 2.507725477218628\n",
      "step: 36900, loss: 2.48774790763855\n",
      "step: 37000, loss: 2.2657504081726074\n",
      "step: 37100, loss: 2.5542006492614746\n",
      "step: 37200, loss: 2.482964038848877\n",
      "step: 37300, loss: 2.480325222015381\n",
      "step: 37400, loss: 2.524177074432373\n",
      "step: 37500, loss: 2.417495012283325\n",
      "step: 37600, loss: 2.4951391220092773\n",
      "step: 37700, loss: 2.5692596435546875\n",
      "step: 37800, loss: 2.3928632736206055\n",
      "step: 37900, loss: 2.478519916534424\n",
      "step: 38000, loss: 2.497889280319214\n",
      "step: 38100, loss: 2.383392572402954\n",
      "step: 38200, loss: 2.5103981494903564\n",
      "step: 38300, loss: 2.5177900791168213\n",
      "step: 38400, loss: 2.489464044570923\n",
      "step: 38500, loss: 2.451631546020508\n",
      "step: 38600, loss: 2.5405523777008057\n",
      "step: 38700, loss: 2.526681423187256\n",
      "step: 38800, loss: 2.445638656616211\n",
      "step: 38900, loss: 2.3832483291625977\n",
      "step: 39000, loss: 2.367027521133423\n",
      "step: 39100, loss: 2.475015163421631\n",
      "step: 39200, loss: 2.378715753555298\n",
      "step: 39300, loss: 2.4539051055908203\n",
      "step: 39400, loss: 2.337627410888672\n",
      "step: 39500, loss: 2.505941390991211\n",
      "step: 39600, loss: 2.4903626441955566\n",
      "step: 39700, loss: 2.5665483474731445\n",
      "step: 39800, loss: 2.373879909515381\n",
      "step: 39900, loss: 2.4930365085601807\n",
      "step: 40000, loss: 2.466078042984009\n",
      "step: 40100, loss: 2.3763463497161865\n",
      "step: 40200, loss: 2.4161248207092285\n",
      "step: 40300, loss: 2.402712106704712\n",
      "step: 40400, loss: 2.5245168209075928\n",
      "step: 40500, loss: 2.448096513748169\n",
      "step: 40600, loss: 2.4088985919952393\n",
      "step: 40700, loss: 2.4748551845550537\n",
      "step: 40800, loss: 2.310878038406372\n",
      "step: 40900, loss: 2.56795334815979\n",
      "step: 41000, loss: 2.390338897705078\n",
      "step: 41100, loss: 2.440948963165283\n",
      "step: 41200, loss: 2.3821845054626465\n",
      "step: 41300, loss: 2.3686177730560303\n",
      "step: 41400, loss: 2.427964687347412\n",
      "step: 41500, loss: 2.4906439781188965\n",
      "step: 41600, loss: 2.4527080059051514\n",
      "step: 41700, loss: 2.3986623287200928\n",
      "step: 41800, loss: 2.3814432621002197\n",
      "step: 41900, loss: 2.524853467941284\n",
      "step: 42000, loss: 2.3883113861083984\n",
      "step: 42100, loss: 2.41646409034729\n",
      "step: 42200, loss: 2.3689396381378174\n",
      "step: 42300, loss: 2.5491108894348145\n",
      "step: 42400, loss: 2.4133591651916504\n",
      "step: 42500, loss: 2.495695114135742\n",
      "step: 42600, loss: 2.419182777404785\n",
      "step: 42700, loss: 2.345632791519165\n",
      "step: 42800, loss: 2.458315134048462\n",
      "step: 42900, loss: 2.3222250938415527\n",
      "step: 43000, loss: 2.3883652687072754\n",
      "step: 43100, loss: 2.6154909133911133\n",
      "step: 43200, loss: 2.409074068069458\n",
      "step: 43300, loss: 2.33516263961792\n",
      "step: 43400, loss: 2.5073015689849854\n",
      "step: 43500, loss: 2.5871379375457764\n",
      "step: 43600, loss: 2.443027973175049\n",
      "step: 43700, loss: 2.404874086380005\n",
      "step: 43800, loss: 2.447355270385742\n",
      "step: 43900, loss: 2.6228768825531006\n",
      "step: 44000, loss: 2.5393636226654053\n",
      "step: 44100, loss: 2.5729570388793945\n",
      "step: 44200, loss: 2.3954250812530518\n",
      "step: 44300, loss: 2.475926160812378\n",
      "step: 44400, loss: 2.3761165142059326\n",
      "step: 44500, loss: 2.303229808807373\n",
      "step: 44600, loss: 2.342604875564575\n",
      "step: 44700, loss: 2.472651958465576\n",
      "step: 44800, loss: 2.5625064373016357\n",
      "step: 44900, loss: 2.6585705280303955\n",
      "step: 45000, loss: 2.409658193588257\n",
      "step: 45100, loss: 2.3670129776000977\n",
      "step: 45200, loss: 2.4170072078704834\n",
      "step: 45300, loss: 2.4813997745513916\n",
      "step: 45400, loss: 2.3808090686798096\n",
      "step: 45500, loss: 2.328226327896118\n",
      "step: 45600, loss: 2.357309341430664\n",
      "step: 45700, loss: 2.316789388656616\n",
      "step: 45800, loss: 2.424917697906494\n",
      "step: 45900, loss: 2.4532008171081543\n",
      "step: 46000, loss: 2.5620405673980713\n",
      "step: 46100, loss: 2.459704637527466\n",
      "step: 46200, loss: 2.4943463802337646\n",
      "step: 46300, loss: 2.4320836067199707\n",
      "step: 46400, loss: 2.560993194580078\n",
      "step: 46500, loss: 2.39050555229187\n",
      "step: 46600, loss: 2.302788734436035\n",
      "step: 46700, loss: 2.460892677307129\n",
      "step: 46800, loss: 2.6046502590179443\n",
      "step: 46900, loss: 2.536491632461548\n",
      "step: 47000, loss: 2.481394052505493\n",
      "step: 47100, loss: 2.4135499000549316\n",
      "step: 47200, loss: 2.3592422008514404\n",
      "step: 47300, loss: 2.4079325199127197\n",
      "step: 47400, loss: 2.4942169189453125\n",
      "step: 47500, loss: 2.428738832473755\n",
      "step: 47600, loss: 2.4974594116210938\n",
      "step: 47700, loss: 2.380037784576416\n",
      "step: 47800, loss: 2.423919677734375\n",
      "step: 47900, loss: 2.4272799491882324\n",
      "step: 48000, loss: 2.42927885055542\n",
      "step: 48100, loss: 2.4797260761260986\n",
      "step: 48200, loss: 2.51997709274292\n",
      "step: 48300, loss: 2.448352336883545\n",
      "step: 48400, loss: 2.489661455154419\n",
      "step: 48500, loss: 2.4385573863983154\n",
      "step: 48600, loss: 2.430263042449951\n",
      "step: 48700, loss: 2.4955060482025146\n",
      "step: 48800, loss: 2.473224401473999\n",
      "step: 48900, loss: 2.433711290359497\n",
      "step: 49000, loss: 2.438575506210327\n",
      "step: 49100, loss: 2.429717779159546\n",
      "step: 49200, loss: 2.4924023151397705\n",
      "step: 49300, loss: 2.439129590988159\n",
      "step: 49400, loss: 2.4618704319000244\n",
      "step: 49500, loss: 2.543292999267578\n",
      "step: 49600, loss: 2.428046941757202\n",
      "step: 49700, loss: 2.4656083583831787\n",
      "step: 49800, loss: 2.4835920333862305\n",
      "step: 49900, loss: 2.4965670108795166\n",
      "step: 50000, loss: 2.5499019622802734\n",
      "step: 50100, loss: 2.5028371810913086\n",
      "step: 50200, loss: 2.535271406173706\n",
      "step: 50300, loss: 2.454286813735962\n",
      "step: 50400, loss: 2.5680384635925293\n",
      "step: 50500, loss: 2.314300537109375\n",
      "step: 50600, loss: 2.361266613006592\n",
      "step: 50700, loss: 2.4781558513641357\n",
      "step: 50800, loss: 2.426785469055176\n",
      "step: 50900, loss: 2.5075955390930176\n",
      "step: 51000, loss: 2.500213146209717\n",
      "step: 51100, loss: 2.483461856842041\n",
      "step: 51200, loss: 2.5008533000946045\n",
      "step: 51300, loss: 2.5335750579833984\n",
      "step: 51400, loss: 2.5682013034820557\n",
      "step: 51500, loss: 2.372152328491211\n",
      "step: 51600, loss: 2.486551284790039\n",
      "step: 51700, loss: 2.3242509365081787\n",
      "step: 51800, loss: 2.365144968032837\n",
      "step: 51900, loss: 2.4715986251831055\n",
      "step: 52000, loss: 2.518716335296631\n",
      "step: 52100, loss: 2.5366179943084717\n",
      "step: 52200, loss: 2.440516710281372\n",
      "step: 52300, loss: 2.4849324226379395\n",
      "step: 52400, loss: 2.4427013397216797\n",
      "step: 52500, loss: 2.409257173538208\n",
      "step: 52600, loss: 2.414752721786499\n",
      "step: 52700, loss: 2.4149584770202637\n",
      "step: 52800, loss: 2.46795916557312\n",
      "step: 52900, loss: 2.316899061203003\n",
      "step: 53000, loss: 2.5402019023895264\n",
      "step: 53100, loss: 2.421086549758911\n",
      "step: 53200, loss: 2.3197097778320312\n",
      "step: 53300, loss: 2.435448169708252\n",
      "step: 53400, loss: 2.3515372276306152\n",
      "step: 53500, loss: 2.393289804458618\n",
      "step: 53600, loss: 2.525463104248047\n",
      "step: 53700, loss: 2.3695337772369385\n",
      "step: 53800, loss: 2.280722141265869\n",
      "step: 53900, loss: 2.3757452964782715\n",
      "step: 54000, loss: 2.442376136779785\n",
      "step: 54100, loss: 2.501375913619995\n",
      "step: 54200, loss: 2.3429808616638184\n",
      "step: 54300, loss: 2.4407894611358643\n",
      "step: 54400, loss: 2.5094282627105713\n",
      "step: 54500, loss: 2.4297049045562744\n",
      "step: 54600, loss: 2.3914132118225098\n",
      "step: 54700, loss: 2.3843562602996826\n",
      "step: 54800, loss: 2.4259843826293945\n",
      "step: 54900, loss: 2.371885061264038\n",
      "step: 55000, loss: 2.594540596008301\n",
      "step: 55100, loss: 2.4153904914855957\n",
      "step: 55200, loss: 2.4498517513275146\n",
      "step: 55300, loss: 2.4400341510772705\n",
      "step: 55400, loss: 2.4207398891448975\n",
      "step: 55500, loss: 2.441680431365967\n",
      "step: 55600, loss: 2.5328829288482666\n",
      "step: 55700, loss: 2.4956886768341064\n",
      "step: 55800, loss: 2.550524950027466\n",
      "step: 55900, loss: 2.286205291748047\n",
      "step: 56000, loss: 2.330231189727783\n",
      "step: 56100, loss: 2.372421979904175\n",
      "step: 56200, loss: 2.3981940746307373\n",
      "step: 56300, loss: 2.5007379055023193\n",
      "step: 56400, loss: 2.4598827362060547\n",
      "step: 56500, loss: 2.438591718673706\n",
      "step: 56600, loss: 2.5482165813446045\n",
      "step: 56700, loss: 2.4127886295318604\n",
      "step: 56800, loss: 2.4834389686584473\n",
      "step: 56900, loss: 2.4252851009368896\n",
      "step: 57000, loss: 2.465937614440918\n",
      "step: 57100, loss: 2.30317759513855\n",
      "step: 57200, loss: 2.38389253616333\n",
      "step: 57300, loss: 2.425910234451294\n",
      "step: 57400, loss: 2.3964221477508545\n",
      "step: 57500, loss: 2.5113368034362793\n",
      "step: 57600, loss: 2.3804705142974854\n",
      "step: 57700, loss: 2.549100399017334\n",
      "step: 57800, loss: 2.447596311569214\n",
      "step: 57900, loss: 2.609086513519287\n",
      "step: 58000, loss: 2.4486539363861084\n",
      "step: 58100, loss: 2.4909698963165283\n",
      "step: 58200, loss: 2.444765567779541\n",
      "step: 58300, loss: 2.371969223022461\n",
      "step: 58400, loss: 2.5248303413391113\n",
      "step: 58500, loss: 2.3756802082061768\n",
      "step: 58600, loss: 2.5319714546203613\n",
      "step: 58700, loss: 2.5865674018859863\n",
      "step: 58800, loss: 2.3343827724456787\n",
      "step: 58900, loss: 2.4418692588806152\n",
      "step: 59000, loss: 2.536161422729492\n",
      "step: 59100, loss: 2.444019317626953\n",
      "step: 59200, loss: 2.486476421356201\n",
      "step: 59300, loss: 2.3121747970581055\n",
      "step: 59400, loss: 2.3818702697753906\n",
      "step: 59500, loss: 2.3749964237213135\n",
      "step: 59600, loss: 2.535372257232666\n",
      "step: 59700, loss: 2.5502898693084717\n",
      "step: 59800, loss: 2.5754923820495605\n",
      "step: 59900, loss: 2.5094261169433594\n",
      "step: 60000, loss: 2.464812755584717\n",
      "step: 60100, loss: 2.501720905303955\n",
      "step: 60200, loss: 2.368135452270508\n",
      "step: 60300, loss: 2.5765717029571533\n",
      "step: 60400, loss: 2.448963165283203\n",
      "step: 60500, loss: 2.5097928047180176\n",
      "step: 60600, loss: 2.471574306488037\n",
      "step: 60700, loss: 2.317483425140381\n",
      "step: 60800, loss: 2.4192049503326416\n",
      "step: 60900, loss: 2.516482353210449\n",
      "step: 61000, loss: 2.407766819000244\n",
      "step: 61100, loss: 2.438965320587158\n",
      "step: 61200, loss: 2.4119176864624023\n",
      "step: 61300, loss: 2.384164571762085\n",
      "step: 61400, loss: 2.3568379878997803\n",
      "step: 61500, loss: 2.4413371086120605\n",
      "step: 61600, loss: 2.508866548538208\n",
      "step: 61700, loss: 2.465524435043335\n",
      "step: 61800, loss: 2.478041172027588\n",
      "step: 61900, loss: 2.5214269161224365\n",
      "step: 62000, loss: 2.480149745941162\n",
      "step: 62100, loss: 2.5030910968780518\n",
      "step: 62200, loss: 2.567091703414917\n",
      "step: 62300, loss: 2.4026172161102295\n",
      "step: 62400, loss: 2.481961488723755\n",
      "step: 62500, loss: 2.4438157081604004\n",
      "step: 62600, loss: 2.364389181137085\n",
      "step: 62700, loss: 2.3633947372436523\n",
      "step: 62800, loss: 2.4147660732269287\n",
      "step: 62900, loss: 2.4378812313079834\n",
      "step: 63000, loss: 2.5057895183563232\n",
      "step: 63100, loss: 2.486926555633545\n",
      "step: 63200, loss: 2.459953546524048\n",
      "step: 63300, loss: 2.4104347229003906\n",
      "step: 63400, loss: 2.546384572982788\n",
      "step: 63500, loss: 2.3467490673065186\n",
      "step: 63600, loss: 2.4340386390686035\n",
      "step: 63700, loss: 2.481672525405884\n",
      "step: 63800, loss: 2.4044766426086426\n",
      "step: 63900, loss: 2.3539042472839355\n",
      "step: 64000, loss: 2.4887948036193848\n",
      "step: 64100, loss: 2.3767647743225098\n",
      "step: 64200, loss: 2.4635210037231445\n",
      "step: 64300, loss: 2.4239485263824463\n",
      "step: 64400, loss: 2.493649482727051\n",
      "step: 64500, loss: 2.5054986476898193\n",
      "step: 64600, loss: 2.4574122428894043\n",
      "step: 64700, loss: 2.359422445297241\n",
      "step: 64800, loss: 2.49265456199646\n",
      "step: 64900, loss: 2.397679090499878\n",
      "step: 65000, loss: 2.3917319774627686\n",
      "step: 65100, loss: 2.4552597999572754\n",
      "step: 65200, loss: 2.444929838180542\n",
      "step: 65300, loss: 2.4920856952667236\n",
      "step: 65400, loss: 2.571718215942383\n",
      "step: 65500, loss: 2.584404945373535\n",
      "step: 65600, loss: 2.390139579772949\n",
      "step: 65700, loss: 2.4197914600372314\n",
      "step: 65800, loss: 2.405801773071289\n",
      "step: 65900, loss: 2.4997525215148926\n",
      "step: 66000, loss: 2.3255162239074707\n",
      "step: 66100, loss: 2.3429882526397705\n",
      "step: 66200, loss: 2.4060323238372803\n",
      "step: 66300, loss: 2.5088915824890137\n",
      "step: 66400, loss: 2.478673219680786\n",
      "step: 66500, loss: 2.483959674835205\n",
      "step: 66600, loss: 2.4269704818725586\n",
      "step: 66700, loss: 2.5256409645080566\n",
      "step: 66800, loss: 2.5302624702453613\n",
      "step: 66900, loss: 2.4846959114074707\n",
      "step: 67000, loss: 2.337100028991699\n",
      "step: 67100, loss: 2.3847146034240723\n",
      "step: 67200, loss: 2.5713541507720947\n",
      "step: 67300, loss: 2.5288679599761963\n",
      "step: 67400, loss: 2.4268181324005127\n",
      "step: 67500, loss: 2.4691781997680664\n",
      "step: 67600, loss: 2.540174961090088\n",
      "step: 67700, loss: 2.3727073669433594\n",
      "step: 67800, loss: 2.284946918487549\n",
      "step: 67900, loss: 2.379138231277466\n",
      "step: 68000, loss: 2.6064600944519043\n",
      "step: 68100, loss: 2.4087514877319336\n",
      "step: 68200, loss: 2.5011093616485596\n",
      "step: 68300, loss: 2.4412894248962402\n",
      "step: 68400, loss: 2.468748092651367\n",
      "step: 68500, loss: 2.3665921688079834\n",
      "step: 68600, loss: 2.603318929672241\n",
      "step: 68700, loss: 2.5304253101348877\n",
      "step: 68800, loss: 2.372424364089966\n",
      "step: 68900, loss: 2.4452695846557617\n",
      "step: 69000, loss: 2.5517923831939697\n",
      "step: 69100, loss: 2.4894607067108154\n",
      "step: 69200, loss: 2.3247647285461426\n",
      "step: 69300, loss: 2.4353508949279785\n",
      "step: 69400, loss: 2.631038188934326\n",
      "step: 69500, loss: 2.5619559288024902\n",
      "step: 69600, loss: 2.4877865314483643\n",
      "step: 69700, loss: 2.4826436042785645\n",
      "step: 69800, loss: 2.467984199523926\n",
      "step: 69900, loss: 2.4406630992889404\n",
      "step: 70000, loss: 2.4461658000946045\n",
      "step: 70100, loss: 2.3373255729675293\n",
      "step: 70200, loss: 2.397024631500244\n",
      "step: 70300, loss: 2.4880146980285645\n",
      "step: 70400, loss: 2.554602861404419\n",
      "step: 70500, loss: 2.510429859161377\n",
      "step: 70600, loss: 2.358250617980957\n",
      "step: 70700, loss: 2.4048123359680176\n",
      "step: 70800, loss: 2.407834768295288\n",
      "step: 70900, loss: 2.426222324371338\n",
      "step: 71000, loss: 2.4515109062194824\n",
      "step: 71100, loss: 2.3852245807647705\n",
      "step: 71200, loss: 2.4342422485351562\n",
      "step: 71300, loss: 2.474362850189209\n",
      "step: 71400, loss: 2.4086732864379883\n",
      "step: 71500, loss: 2.496929883956909\n",
      "step: 71600, loss: 2.51491379737854\n",
      "step: 71700, loss: 2.4462695121765137\n",
      "step: 71800, loss: 2.4353692531585693\n",
      "step: 71900, loss: 2.4345996379852295\n",
      "step: 72000, loss: 2.4663939476013184\n",
      "step: 72100, loss: 2.4566256999969482\n",
      "step: 72200, loss: 2.4157416820526123\n",
      "step: 72300, loss: 2.514207601547241\n",
      "step: 72400, loss: 2.4655115604400635\n",
      "step: 72500, loss: 2.425950050354004\n",
      "step: 72600, loss: 2.3449854850769043\n",
      "step: 72700, loss: 2.3502564430236816\n",
      "step: 72800, loss: 2.426981210708618\n",
      "step: 72900, loss: 2.56210994720459\n",
      "step: 73000, loss: 2.3224174976348877\n",
      "step: 73100, loss: 2.451113224029541\n",
      "step: 73200, loss: 2.395876884460449\n",
      "step: 73300, loss: 2.520799398422241\n",
      "step: 73400, loss: 2.3533360958099365\n",
      "step: 73500, loss: 2.4987432956695557\n",
      "step: 73600, loss: 2.545889377593994\n",
      "step: 73700, loss: 2.4430880546569824\n",
      "step: 73800, loss: 2.491623878479004\n",
      "step: 73900, loss: 2.541996955871582\n",
      "step: 74000, loss: 2.466837167739868\n",
      "step: 74100, loss: 2.385427236557007\n",
      "step: 74200, loss: 2.4333245754241943\n",
      "step: 74300, loss: 2.480907440185547\n",
      "step: 74400, loss: 2.512331485748291\n",
      "step: 74500, loss: 2.582693576812744\n",
      "step: 74600, loss: 2.5118625164031982\n",
      "step: 74700, loss: 2.4366655349731445\n",
      "step: 74800, loss: 2.4704489707946777\n",
      "step: 74900, loss: 2.421177625656128\n",
      "step: 75000, loss: 2.631845712661743\n",
      "step: 75100, loss: 2.3646364212036133\n",
      "step: 75200, loss: 2.5073459148406982\n",
      "step: 75300, loss: 2.515808343887329\n",
      "step: 75400, loss: 2.379150867462158\n",
      "step: 75500, loss: 2.506286144256592\n",
      "step: 75600, loss: 2.46419358253479\n",
      "step: 75700, loss: 2.3477134704589844\n",
      "step: 75800, loss: 2.4777121543884277\n",
      "step: 75900, loss: 2.4627482891082764\n",
      "step: 76000, loss: 2.404773712158203\n",
      "step: 76100, loss: 2.435438632965088\n",
      "step: 76200, loss: 2.54628586769104\n",
      "step: 76300, loss: 2.6298210620880127\n",
      "step: 76400, loss: 2.5939228534698486\n",
      "step: 76500, loss: 2.4150567054748535\n",
      "step: 76600, loss: 2.4556386470794678\n",
      "step: 76700, loss: 2.422743558883667\n",
      "step: 76800, loss: 2.43753719329834\n",
      "step: 76900, loss: 2.4632480144500732\n",
      "step: 77000, loss: 2.514601230621338\n",
      "step: 77100, loss: 2.437732219696045\n",
      "step: 77200, loss: 2.5218985080718994\n",
      "step: 77300, loss: 2.6067094802856445\n",
      "step: 77400, loss: 2.513613224029541\n",
      "step: 77500, loss: 2.419938087463379\n",
      "step: 77600, loss: 2.427072525024414\n",
      "step: 77700, loss: 2.3761720657348633\n",
      "step: 77800, loss: 2.3886940479278564\n",
      "step: 77900, loss: 2.5950191020965576\n",
      "step: 78000, loss: 2.390657424926758\n",
      "step: 78100, loss: 2.4519033432006836\n",
      "step: 78200, loss: 2.489013910293579\n",
      "step: 78300, loss: 2.4405460357666016\n",
      "step: 78400, loss: 2.5014281272888184\n",
      "step: 78500, loss: 2.5057566165924072\n",
      "step: 78600, loss: 2.3091769218444824\n",
      "step: 78700, loss: 2.436790943145752\n",
      "step: 78800, loss: 2.4946558475494385\n",
      "step: 78900, loss: 2.3494017124176025\n",
      "step: 79000, loss: 2.4574878215789795\n",
      "step: 79100, loss: 2.508847236633301\n",
      "step: 79200, loss: 2.4405031204223633\n",
      "step: 79300, loss: 2.384162187576294\n",
      "step: 79400, loss: 2.507746458053589\n",
      "step: 79500, loss: 2.3571465015411377\n",
      "step: 79600, loss: 2.485074281692505\n",
      "step: 79700, loss: 2.4759654998779297\n",
      "step: 79800, loss: 2.511007070541382\n",
      "step: 79900, loss: 2.339825391769409\n",
      "step: 80000, loss: 2.4025893211364746\n",
      "step: 80100, loss: 2.447925329208374\n",
      "step: 80200, loss: 2.5214576721191406\n",
      "step: 80300, loss: 2.4581480026245117\n",
      "step: 80400, loss: 2.4790732860565186\n",
      "step: 80500, loss: 2.4738197326660156\n",
      "step: 80600, loss: 2.4225270748138428\n",
      "step: 80700, loss: 2.4582085609436035\n",
      "step: 80800, loss: 2.476747989654541\n",
      "step: 80900, loss: 2.549990177154541\n",
      "step: 81000, loss: 2.5616047382354736\n",
      "step: 81100, loss: 2.464731216430664\n",
      "step: 81200, loss: 2.5692591667175293\n",
      "step: 81300, loss: 2.597198247909546\n",
      "step: 81400, loss: 2.6174705028533936\n",
      "step: 81500, loss: 2.3961398601531982\n",
      "step: 81600, loss: 2.486443281173706\n",
      "step: 81700, loss: 2.539879322052002\n",
      "step: 81800, loss: 2.4148521423339844\n",
      "step: 81900, loss: 2.550849199295044\n",
      "step: 82000, loss: 2.3612122535705566\n",
      "step: 82100, loss: 2.5134143829345703\n",
      "step: 82200, loss: 2.4783213138580322\n",
      "step: 82300, loss: 2.4409241676330566\n",
      "step: 82400, loss: 2.4335100650787354\n",
      "step: 82500, loss: 2.547288417816162\n",
      "step: 82600, loss: 2.3772544860839844\n",
      "step: 82700, loss: 2.526341438293457\n",
      "step: 82800, loss: 2.3809449672698975\n",
      "step: 82900, loss: 2.442983388900757\n",
      "step: 83000, loss: 2.3272907733917236\n",
      "step: 83100, loss: 2.4765565395355225\n",
      "step: 83200, loss: 2.462071657180786\n",
      "step: 83300, loss: 2.588801622390747\n",
      "step: 83400, loss: 2.367750644683838\n",
      "step: 83500, loss: 2.4244301319122314\n",
      "step: 83600, loss: 2.4729654788970947\n",
      "step: 83700, loss: 2.4644837379455566\n",
      "step: 83800, loss: 2.5291059017181396\n",
      "step: 83900, loss: 2.400364875793457\n",
      "step: 84000, loss: 2.305943727493286\n",
      "step: 84100, loss: 2.462409496307373\n",
      "step: 84200, loss: 2.442857027053833\n",
      "step: 84300, loss: 2.4841973781585693\n",
      "step: 84400, loss: 2.4897797107696533\n",
      "step: 84500, loss: 2.482208251953125\n",
      "step: 84600, loss: 2.4663145542144775\n",
      "step: 84700, loss: 2.486255168914795\n",
      "step: 84800, loss: 2.364138126373291\n",
      "step: 84900, loss: 2.3703927993774414\n",
      "step: 85000, loss: 2.5765488147735596\n",
      "step: 85100, loss: 2.4285998344421387\n",
      "step: 85200, loss: 2.467529058456421\n",
      "step: 85300, loss: 2.4355416297912598\n",
      "step: 85400, loss: 2.4580061435699463\n",
      "step: 85500, loss: 2.4634265899658203\n",
      "step: 85600, loss: 2.402305841445923\n",
      "step: 85700, loss: 2.4634244441986084\n",
      "step: 85800, loss: 2.422609329223633\n",
      "step: 85900, loss: 2.585392475128174\n",
      "step: 86000, loss: 2.474607229232788\n",
      "step: 86100, loss: 2.3981072902679443\n",
      "step: 86200, loss: 2.5560858249664307\n",
      "step: 86300, loss: 2.423560619354248\n",
      "step: 86400, loss: 2.4404215812683105\n",
      "step: 86500, loss: 2.3663713932037354\n",
      "step: 86600, loss: 2.4067068099975586\n",
      "step: 86700, loss: 2.428010940551758\n",
      "step: 86800, loss: 2.4366114139556885\n",
      "step: 86900, loss: 2.4318573474884033\n",
      "step: 87000, loss: 2.353323459625244\n",
      "step: 87100, loss: 2.3630661964416504\n",
      "step: 87200, loss: 2.5067057609558105\n",
      "step: 87300, loss: 2.458996534347534\n",
      "step: 87400, loss: 2.4417314529418945\n",
      "step: 87500, loss: 2.4051706790924072\n",
      "step: 87600, loss: 2.3764522075653076\n",
      "step: 87700, loss: 2.4211184978485107\n",
      "step: 87800, loss: 2.364140510559082\n",
      "step: 87900, loss: 2.4695558547973633\n",
      "step: 88000, loss: 2.5019466876983643\n",
      "step: 88100, loss: 2.483912706375122\n",
      "step: 88200, loss: 2.4624805450439453\n",
      "step: 88300, loss: 2.5045390129089355\n",
      "step: 88400, loss: 2.4533050060272217\n",
      "step: 88500, loss: 2.6625802516937256\n",
      "step: 88600, loss: 2.4559876918792725\n",
      "step: 88700, loss: 2.4559426307678223\n",
      "step: 88800, loss: 2.5218517780303955\n",
      "step: 88900, loss: 2.4177303314208984\n",
      "step: 89000, loss: 2.3863964080810547\n",
      "step: 89100, loss: 2.461798906326294\n",
      "step: 89200, loss: 2.389206647872925\n",
      "step: 89300, loss: 2.5037331581115723\n",
      "step: 89400, loss: 2.4723289012908936\n",
      "step: 89500, loss: 2.3569958209991455\n",
      "step: 89600, loss: 2.5204668045043945\n",
      "step: 89700, loss: 2.6542978286743164\n",
      "step: 89800, loss: 2.467376470565796\n",
      "step: 89900, loss: 2.362948417663574\n",
      "step: 90000, loss: 2.5268807411193848\n",
      "step: 90100, loss: 2.308955430984497\n",
      "step: 90200, loss: 2.4533791542053223\n",
      "step: 90300, loss: 2.5781586170196533\n",
      "step: 90400, loss: 2.5060057640075684\n",
      "step: 90500, loss: 2.3974549770355225\n",
      "step: 90600, loss: 2.499645709991455\n",
      "step: 90700, loss: 2.511430501937866\n",
      "step: 90800, loss: 2.3685452938079834\n",
      "step: 90900, loss: 2.38150954246521\n",
      "step: 91000, loss: 2.485034227371216\n",
      "step: 91100, loss: 2.3376591205596924\n",
      "step: 91200, loss: 2.519838571548462\n",
      "step: 91300, loss: 2.375786781311035\n",
      "step: 91400, loss: 2.53068208694458\n",
      "step: 91500, loss: 2.464444637298584\n",
      "step: 91600, loss: 2.427734613418579\n",
      "step: 91700, loss: 2.433924436569214\n",
      "step: 91800, loss: 2.4537906646728516\n",
      "step: 91900, loss: 2.5408012866973877\n",
      "step: 92000, loss: 2.4432625770568848\n",
      "step: 92100, loss: 2.4139256477355957\n",
      "step: 92200, loss: 2.5342721939086914\n",
      "step: 92300, loss: 2.4575932025909424\n",
      "step: 92400, loss: 2.515578508377075\n",
      "step: 92500, loss: 2.3801445960998535\n",
      "step: 92600, loss: 2.601567029953003\n",
      "step: 92700, loss: 2.398066282272339\n",
      "step: 92800, loss: 2.4767112731933594\n",
      "step: 92900, loss: 2.3021039962768555\n",
      "step: 93000, loss: 2.4989752769470215\n",
      "step: 93100, loss: 2.559434413909912\n",
      "step: 93200, loss: 2.454885959625244\n",
      "step: 93300, loss: 2.4149694442749023\n",
      "step: 93400, loss: 2.6276707649230957\n",
      "step: 93500, loss: 2.5178616046905518\n",
      "step: 93600, loss: 2.431621789932251\n",
      "step: 93700, loss: 2.40602445602417\n",
      "step: 93800, loss: 2.3077664375305176\n",
      "step: 93900, loss: 2.4956917762756348\n",
      "step: 94000, loss: 2.49935245513916\n",
      "step: 94100, loss: 2.4202029705047607\n",
      "step: 94200, loss: 2.418621063232422\n",
      "step: 94300, loss: 2.4894042015075684\n",
      "step: 94400, loss: 2.445324420928955\n",
      "step: 94500, loss: 2.465669870376587\n",
      "step: 94600, loss: 2.4015040397644043\n",
      "step: 94700, loss: 2.4578518867492676\n",
      "step: 94800, loss: 2.4381253719329834\n",
      "step: 94900, loss: 2.471574544906616\n",
      "step: 95000, loss: 2.4788904190063477\n",
      "step: 95100, loss: 2.449821949005127\n",
      "step: 95200, loss: 2.514324426651001\n",
      "step: 95300, loss: 2.5075244903564453\n",
      "step: 95400, loss: 2.449519157409668\n",
      "step: 95500, loss: 2.3277955055236816\n",
      "step: 95600, loss: 2.4264938831329346\n",
      "step: 95700, loss: 2.4147043228149414\n",
      "step: 95800, loss: 2.359323263168335\n",
      "step: 95900, loss: 2.416232109069824\n",
      "step: 96000, loss: 2.442213296890259\n",
      "step: 96100, loss: 2.639657497406006\n",
      "step: 96200, loss: 2.5009400844573975\n",
      "step: 96300, loss: 2.4949393272399902\n",
      "step: 96400, loss: 2.3519093990325928\n",
      "step: 96500, loss: 2.375699043273926\n",
      "step: 96600, loss: 2.446404457092285\n",
      "step: 96700, loss: 2.4022443294525146\n",
      "step: 96800, loss: 2.431166887283325\n",
      "step: 96900, loss: 2.451082229614258\n",
      "step: 97000, loss: 2.3599724769592285\n",
      "step: 97100, loss: 2.470292806625366\n",
      "step: 97200, loss: 2.451508045196533\n",
      "step: 97300, loss: 2.4625303745269775\n",
      "step: 97400, loss: 2.475048780441284\n",
      "step: 97500, loss: 2.4036777019500732\n",
      "step: 97600, loss: 2.4223780632019043\n",
      "step: 97700, loss: 2.359095573425293\n",
      "step: 97800, loss: 2.4556362628936768\n",
      "step: 97900, loss: 2.496706962585449\n",
      "step: 98000, loss: 2.3962666988372803\n",
      "step: 98100, loss: 2.489461660385132\n",
      "step: 98200, loss: 2.415424346923828\n",
      "step: 98300, loss: 2.44990873336792\n",
      "step: 98400, loss: 2.543574333190918\n",
      "step: 98500, loss: 2.536341667175293\n",
      "step: 98600, loss: 2.393463373184204\n",
      "step: 98700, loss: 2.4496333599090576\n",
      "step: 98800, loss: 2.4105637073516846\n",
      "step: 98900, loss: 2.3723671436309814\n",
      "step: 99000, loss: 2.461430072784424\n",
      "step: 99100, loss: 2.3444883823394775\n",
      "step: 99200, loss: 2.2513792514801025\n",
      "step: 99300, loss: 2.457346200942993\n",
      "step: 99400, loss: 2.4214212894439697\n",
      "step: 99500, loss: 2.582691192626953\n",
      "step: 99600, loss: 2.391941547393799\n",
      "step: 99700, loss: 2.599043607711792\n",
      "step: 99800, loss: 2.5400891304016113\n",
      "step: 99900, loss: 2.4408278465270996\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100000):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 100 == 0:\n",
    "        print(f'step: {steps}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K:\n",
      "Thel BExckngl e wante buce be g scheeaneanefr qupore.\n",
      "Hiceshoou dur lllowisat,\n",
      "Brull d age arere we, that,\n",
      "Wh atthe I\n",
      "\n",
      "Hor thal, thing t d.\n",
      "K:\n",
      "Ann fr:\n",
      "\n",
      "D:\n",
      "SINor WAs d berenen! a ito tithaferot! ortoowilor fethouse;\n",
      "ave uf e ldydofomangou: fodiereindsllly RCKim\n",
      "TES:\n",
      "\n",
      "Twhano e h s'dr y towatin bit do, tthinatathaid athr:\n",
      "DULAGBy a bl:\n",
      "Bour sar hos s ten s afeshe?\n",
      "Tw, thy lar d won l ve t, mbe ce he d at ICO:\n",
      "\n",
      "Thangllsl s aveloutr usulde prded frdsatheatorin an Drspo pomaced athid my ourtat chen\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, 500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
