{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = torch.device('mps')\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "eval_iters = 2000\n",
    "n_embd = 384\n",
    "n_layers = 6\n",
    "dropout = 0.2\n",
    "n_head = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 17, 24, 24, 27]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "def encode(s): return [stoi[ch] for ch in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "# Split in to train anv validation\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "print(encode(\"HELLO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module):  # not actually a bigram model anymore\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_layers) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.final_ln = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensors. where B is batch size and T is the number of tokens in each sequence (block_size*batch_size)\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.final_ln(x) # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,V)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=-1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''a single self attention head'''\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)  # (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B,T,C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        # Q. what is proj an abbreviation for? A. projection Q. What is meant by projection? A. I think it means that the output is the same size as the input.3\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)  # (B,T,C)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "\n",
    "        xmean = x.mean(1, keepdim=True)  # batch mean\n",
    "        xvar = x.var(1, keepdim=True)  # batch variance\n",
    "\n",
    "        # normalize to unit variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 10788929\n"
     ]
    }
   ],
   "source": [
    "m = Transformer().to(device)\n",
    "print(f'Number of parameters: {sum(p.numel() for p in m.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 4.365875720977783, time taken: 0:00:02.197683\n",
      "estimated time remaining: 3:03:08.415480\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 16\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m lossi\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mlog10()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
    "lossi = []\n",
    "start_time = time.time()  # Record the starting time\n",
    "\n",
    "for steps in range(max_iters):\n",
    "    xb, yb = get_batch('train')\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "    if steps % eval_interval == 0:\n",
    "        current_time = time.time() - start_time\n",
    "        formatted_time = str(datetime.timedelta(seconds=current_time))\n",
    "        print(f'step: {steps}, loss: {loss.item()}, time taken: {formatted_time}')\n",
    "\n",
    "        # Estimate remaining time\n",
    "        steps_remaining = max_iters - steps\n",
    "        time_per_step = current_time / (steps + 1)  # Avoid division by zero\n",
    "        remaining_time = datetime.timedelta(seconds=steps_remaining * time_per_step)\n",
    "        print(f'estimated time remaining: {remaining_time}')\n",
    "\n",
    "# Optionally, you can print the total training time\n",
    "total_time = time.time() - start_time\n",
    "formatted_total_time = str(datetime.timedelta(seconds=total_time))\n",
    "print(f'Total training time: {formatted_total_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "lossi_means = [sum(lossi[i:i+100])/100 for i in range(len(lossi)-100)]\n",
    "plt.plot(lossi_means)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device), yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39meval_iters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[139], line 20\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     17\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(\n\u001b[1;32m     18\u001b[0m     torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))  \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_ln(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)  \u001b[38;5;66;03m# (B,T,V)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[139], line 110\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 110\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[139], line 82\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     80\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([head(x) \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out)\n\u001b[0;32m---> 82\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluation loop over validation set\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for steps in range(eval_iters):\n",
    "        xb, yb = get_batch('val')\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits, loss = m(xb, yb)\n",
    "        total_loss += loss.item()\n",
    "    print(f'validation loss: {total_loss/eval_iters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "tensor([13], device='mps:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate some text\u001b[39;00m\n\u001b[1;32m      2\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode(\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "Cell \u001b[0;32mIn[132], line 42\u001b[0m, in \u001b[0;36mTransformer.generate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m     40\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m     idx_next \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28mprint\u001b[39m([\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_next\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m     43\u001b[0m     idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((idx, idx_next), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m idx\n",
      "Cell \u001b[0;32mIn[89], line 9\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(l)\u001b[0m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(l): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43m[\u001b[49m\u001b[43mitos\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m)\n",
      "Cell \u001b[0;32mIn[89], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(l): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[43mitos\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m l])\n",
      "\u001b[0;31mKeyError\u001b[0m: tensor([13], device='mps:0')"
     ]
    }
   ],
   "source": [
    "# Generate some text\n",
    "idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "print(decode(m.generate(idx, 500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "torch.save(m.state_dict(), 'model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_embedding_table): Embedding(65, 384)\n",
       "  (position_embedding_table): Embedding(256, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load model_finalvideo_shakespeare.py\n",
    "m = Transformer()\n",
    "# Load the saved model\n",
    "m.load_state_dict(torch.load('model_finalvideo_whatsapp.pt', map_location='cpu'))\n",
    "m.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Richard! Cominius is in the steal of hit?\n",
      "By a whole the bolts can for out of York.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "I have it; and even in thee to breath.\n",
      "\n",
      "BINCAKE.\n",
      "\n",
      "CLIFFORD:\n",
      "Ay, do not bury thy death, as I am.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Your propheciously riddles wreck,\n",
      "I love your formacions obsetimes\n",
      "To stir legs of held that knave my strong'd Gline\n",
      "My Duch or and the practise of Richard isle;\n",
      "For that, use 'twere defence is a cross' death:\n",
      "But cannot my love; and the wife the will abound\n",
      "In justify that rid that barbaring me.\n",
      "The manners now are merry, nursed up Hermy.\n",
      "\n",
      "KING HENRY VI:\n",
      "Marry, Warwick, be taken to me, faith.\n",
      "\n",
      "KING EDWARD IV:\n",
      "And what say they say to leanness us too rose!\n",
      "But they sague: heaven fled upon their is!\n",
      "\n",
      "GLOUCESTER:\n",
      "Pal"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(new_chars):\n\u001b[0;32m----> 5\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      6\u001b[0m     decoded_output \u001b[38;5;241m=\u001b[39m decode(output)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(decoded_output[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[139], line 38\u001b[0m, in \u001b[0;36mTransformer.generate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# crop idx\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     idx_cond \u001b[38;5;241m=\u001b[39m idx[:, \u001b[38;5;241m-\u001b[39mblock_size:]\n\u001b[0;32m---> 38\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     40\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[139], line 20\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     17\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(\n\u001b[1;32m     18\u001b[0m     torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))  \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_ln(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)  \u001b[38;5;66;03m# (B,T,V)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[139], line 110\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 110\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[139], line 80\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 80\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out)\n\u001b[1;32m     82\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n",
      "Cell \u001b[0;32mIn[139], line 80\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 80\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out)\n\u001b[1;32m     82\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[139], line 66\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m wei \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(wei, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B,T,T)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(wei)\n\u001b[0;32m---> 66\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m out \u001b[38;5;241m=\u001b[39m wei \u001b[38;5;241m@\u001b[39m v  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_chars = 10000\n",
    "idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "\n",
    "for _ in range(new_chars):\n",
    "    output = m.generate(idx, 1)[0].tolist()\n",
    "    decoded_output = decode(output)\n",
    "    print(decoded_output[-1], end='')\n",
    "    idx = torch.tensor([output], dtype=torch.long).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
