{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 \n",
    "block_size = 256 # spatial extent of the model for its context\n",
    "max_iters = 5000 # number of training iterations\n",
    "eval_interval = 500 # frequency of printing training stats\n",
    "learning_rate = 3e-4\n",
    "device = torch.device('mps')\n",
    "eval_iters = 200 # number of iterations to evaluate the model\n",
    "n_embd = 384\n",
    "n_layers = 6\n",
    "dropout = 0.2\n",
    "n_head = 6\n",
    "data_filepath = 'commedia.txt'\n",
    "train_val_split = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Length of train data: 170866\n",
      " - Length of val data: 8993\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data\n",
    "with open(data_filepath, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)} # string to int\n",
    "itos = {i: ch for i, ch in enumerate(chars)} # int to string\n",
    "def encode(s): return [stoi[ch] for ch in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Split in to train anv validation\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(train_val_split * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f' - Length of train data: {len(train_data)}')\n",
    "print(f' - Length of val data: {len(val_data)}')\n",
    "\n",
    "def get_batch(split, batch_size, block_size):\n",
    "    \"\"\"\n",
    "    Get a batch of data for training or validation.\n",
    "\n",
    "    Parameters:\n",
    "    split (str): The split to get the data from. Can be 'train' or 'val'.\n",
    "    batch_size (int): The batch size.\n",
    "    block_size (int): The sequence length.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor, torch.Tensor: The input data (x) and target data (y) as tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module): \n",
    "    ''' A transformer language model. Parameters are defined in the hyperparameters section and do not need to be passed in.\n",
    "    The class has two methods: forward and generate. Forward is used for training and generate is used for sampling.\n",
    "    In the forward method, the input is a batch of sequences of tokens, and optionally a batch of target sequences of tokens.'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_layers) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.final_ln = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        ''' idx is a batch of sequences of tokens. targets is a batch of target sequences of tokens. If targets is None, then the loss is not calculated.'''\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensors. where B is batch size and T is the number of tokens in each sequence (block_size*batch_size)\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.final_ln(x) # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,V)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        ''' idx is a batch of sequences of tokens. max_new_tokens is the maximum number of tokens to generate.'''\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=-1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''a single self attention head'''\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)  # (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B,T,C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        # Q. what is proj an abbreviation for? A. projection Q. What is meant by projection? A. I think it means that the output is the same size as the input.3\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)  # (B,T,C)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "\n",
    "        xmean = x.mean(1, keepdim=True)  # batch mean\n",
    "        xvar = x.var(1, keepdim=True)  # batch variance\n",
    "\n",
    "        # normalize to unit variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is running on mps\n",
      "Number of parameters: 10785084\n"
     ]
    }
   ],
   "source": [
    "m = Transformer().to(device)\n",
    "print(f'Model is running on {device}')\n",
    "print(f'Number of parameters: {sum(p.numel() for p in m.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.27749037742614746, time taken: 0:00:51.520613\n",
      "estimated time remaining: 2 days, 23:33:23.065968\n",
      "step: 500, loss: 0.23571012914180756, time taken: 0:11:33.767167\n",
      "estimated time remaining: 1:43:51.441621\n",
      "step: 1000, loss: 0.19895993173122406, time taken: 0:22:12.680410\n",
      "estimated time remaining: 1:28:45.396244\n",
      "step: 1500, loss: 0.18962806463241577, time taken: 0:32:54.211022\n",
      "estimated time remaining: 1:16:43.423436\n",
      "step: 2000, loss: 0.1612018197774887, time taken: 0:43:52.869746\n",
      "estimated time remaining: 1:05:47.330953\n",
      "step: 2500, loss: 0.16034285724163055, time taken: 0:54:42.986093\n",
      "estimated time remaining: 0:54:41.673424\n",
      "step: 3000, loss: 0.15108683705329895, time taken: 1:05:47.757806\n",
      "estimated time remaining: 0:43:50.961550\n",
      "step: 3500, loss: 0.1387234479188919, time taken: 1:16:47.657694\n",
      "estimated time remaining: 0:32:54.146398\n",
      "step: 4000, loss: 0.13468334078788757, time taken: 1:27:34.622010\n",
      "estimated time remaining: 0:21:53.327171\n",
      "step: 4500, loss: 0.12445821613073349, time taken: 1:38:46.839721\n",
      "estimated time remaining: 0:10:58.391438\n",
      "Total training time: 1:50:11.428327\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate) \n",
    "lossi = [] # loss history\n",
    "start_time = time.time() \n",
    "\n",
    "for steps in range(max_iters):\n",
    "    xb, yb = get_batch('train')\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "    if steps % eval_interval == 0 or steps == max_iters - 1:\n",
    "        current_time = time.time() - start_time\n",
    "        formatted_time = str(datetime.timedelta(seconds=current_time))\n",
    "        print(f'step: {steps}, loss: {loss.item()}, time taken: {formatted_time}')\n",
    "\n",
    "        # Estimate remaining time\n",
    "        steps_remaining = max_iters - steps\n",
    "        time_per_step = current_time / (steps + 1)  # Avoid division by zero\n",
    "        remaining_time = datetime.timedelta(seconds=steps_remaining * time_per_step)\n",
    "        print(f'estimated time remaining: {remaining_time}')\n",
    "\n",
    "# Print the total training time\n",
    "total_time = time.time() - start_time\n",
    "formatted_total_time = str(datetime.timedelta(seconds=total_time))\n",
    "print(f'Total training time: {formatted_total_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "lossi_means = [sum(lossi[i:i+100])/100 for i in range(len(lossi)-100)]\n",
    "plt.plot(lossi_means)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[309], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device), yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39meval_iters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[251], line 20\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     17\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(\n\u001b[1;32m     18\u001b[0m     torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))  \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_ln(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)  \u001b[38;5;66;03m# (B,T,V)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[251], line 110\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 110\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[251], line 80\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 80\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out)\n\u001b[1;32m     82\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n",
      "Cell \u001b[0;32mIn[251], line 80\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 80\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out)\n\u001b[1;32m     82\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[251], line 63\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(x)  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m wei \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m (C \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)  \u001b[38;5;66;03m# (B,T,T)\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[43mwei\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtril\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-inf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m wei \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(wei, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B,T,T)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(wei)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluation loop over validation set\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for steps in range(eval_iters):\n",
    "        xb, yb = get_batch('val')\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits, loss = m(xb, yb)\n",
    "        total_loss += loss.item()\n",
    "    print(f'validation loss: {total_loss/eval_iters}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "che la prima avea vea enaea luca:\n",
      "\"O virte poeta voler cio` che s'altra,\n",
      "se 'l vene ogne simile anzia la coda?\n",
      "\n",
      "Per che 'l duca mio a guardar piu` di morti,"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[325], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(new_chars):\n\u001b[0;32m----> 5\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      6\u001b[0m     decoded_output \u001b[38;5;241m=\u001b[39m decode(output)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(decoded_output[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[251], line 38\u001b[0m, in \u001b[0;36mTransformer.generate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# crop idx\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     idx_cond \u001b[38;5;241m=\u001b[39m idx[:, \u001b[38;5;241m-\u001b[39mblock_size:]\n\u001b[0;32m---> 38\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     40\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[251], line 20\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     17\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(\n\u001b[1;32m     18\u001b[0m     torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))  \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_ln(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)  \u001b[38;5;66;03m# (B,T,V)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[251], line 110\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 110\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[251], line 80\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 80\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out)\n\u001b[1;32m     82\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n",
      "Cell \u001b[0;32mIn[251], line 80\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 80\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out)\n\u001b[1;32m     82\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[251], line 63\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(x)  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m wei \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m (C \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)  \u001b[38;5;66;03m# (B,T,T)\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m wei \u001b[38;5;241m=\u001b[39m wei\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtril\u001b[49m[:T, :T] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     64\u001b[0m wei \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(wei, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B,T,T)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(wei)\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1682\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1683\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1684\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate from the model, printing the output as it goes\n",
    "new_chars = 10000\n",
    "idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "\n",
    "for _ in range(new_chars):\n",
    "    output = m.generate(idx, 1)[0].tolist()\n",
    "    decoded_output = decode(output)\n",
    "    print(decoded_output[-1], end='')\n",
    "    idx = torch.tensor([output], dtype=torch.long).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Time printing 100 characters on mps and cpu\n",
    "# # MPS\n",
    "# import time\n",
    "# t0 = time.time()\n",
    "# new_chars = 100\n",
    "# idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "\n",
    "# for _ in range(new_chars):\n",
    "#     output = m.generate(idx, 1)[0].tolist()\n",
    "#     decoded_output = decode(output)\n",
    "#     print(decoded_output[-1], end='')\n",
    "#     idx = torch.tensor([output], dtype=torch.long).to(device)\n",
    "# t1 = time.time()\n",
    "# print('\\ntime taken: ', t1-t0)\n",
    "\n",
    "# # CPU\n",
    "# device = torch.device('mps')\n",
    "# m = Transformer()\n",
    "# # Load the saved model\n",
    "# m.load_state_dict(torch.load('model_finalvideo_shakespeare.pt', map_location='cpu'))\n",
    "# m.to(device)\n",
    "# import time\n",
    "# t0 = time.time()\n",
    "# new_chars = 100\n",
    "# idx = torch.zeros((1, 1), dtype=torch.long).to('mps')\n",
    "\n",
    "# for _ in range(new_chars):\n",
    "#     output = m.generate(idx, 1)[0].tolist()\n",
    "#     decoded_output = decode(output)\n",
    "#     print(decoded_output[-1], end='')\n",
    "#     idx = torch.tensor([output], dtype=torch.long).to(device)\n",
    "\n",
    "# t1 = time.time()\n",
    "# print('\\ntime taken: ', t1-t0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will do the following: generate 500 tokens, then go through the generate text and take every three word group, and see if we can find it in the original text\n",
    "idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "test_output = decode(m.generate(idx, 5000)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"per che la\" found in text\n",
      "\"ond' io mi\" found in text\n",
      "\"in giuso e`\" found in text\n",
      "\"giuso e` tutto\" found in text\n",
      "\"e` tutto ferro\" found in text\n",
      "\"tutto ferro eletto,\" found in text\n",
      "\"salvo che 'l\" found in text\n",
      "\"che 'l destro\" found in text\n",
      "\"'l destro piede\" found in text\n",
      "\"destro piede e`\" found in text\n",
      "\"quel che tu\" found in text\n",
      "\"che tu vuoli\";\" found in text\n",
      "\"i per la\" found in text\n",
      "\"per la gran\" found in text\n",
      "\"Allor lo presi\" found in text\n",
      "\"a me che\" found in text\n",
      "\"il primo canto\" found in text\n",
      "\"la colpa che\" found in text\n",
      "\"che 'l morde\" found in text\n",
      "\"di qua, di\" found in text\n",
      "\"qua, di la`,\" found in text\n",
      "\"di la`, e\" found in text\n",
      "\"la`, e poi\" found in text\n",
      "\"forza ne la\" found in text\n",
      "\"la terra che\" found in text\n",
      "\"terra che fe'\" found in text\n",
      "\"che fe' gia`\" found in text\n",
      "\"disse 'l mio\" found in text\n",
      "\"'l mio duca,\" found in text\n",
      "\"che non li\" found in text\n",
      "\"non li ha,\" found in text\n",
      "\"fe' gia` la\" found in text\n",
      "\"ne la sua\" found in text\n",
      "\"e io e\" found in text\n",
      "\"su per l'ombre\" found in text\n",
      "\"e qual mozzo\" found in text\n",
      "\"e io a\" found in text\n",
      "\"io a dir\" found in text\n",
      "\"la colpa che\" found in text\n",
      "\"che 'l mondo\" found in text\n",
      "\"e 'l gran\" found in text\n",
      "\"che del futuro\" found in text\n",
      "\"del futuro mi\" found in text\n",
      "\"io aspetto i\" found in text\n",
      "\"e ciascun era\" found in text\n",
      "\"surge e l'altra\" found in text\n",
      "\"quando un altro\" found in text\n",
      "\"che da l'un\" found in text\n",
      "\"e che 'l\" found in text\n",
      "\"che 'l vento\" found in text\n",
      "\"che gia` non\" found in text\n",
      "\"con li occhi\" found in text\n",
      "\"li occhi suoi\" found in text\n",
      "\"per lo scoglio\" found in text\n",
      "\"e li orecchi\" found in text\n",
      "\"e grido`: \"Qual\" found in text\n",
      "\"grido`: \"Qual maraviglia!\".\" found in text\n",
      "\"E io, quando\" found in text\n",
      "\"io, quando 'l\" found in text\n",
      "\"quando 'l suo\" found in text\n",
      "\"'l suo braccio\" found in text\n",
      "\"suo braccio a\" found in text\n",
      "\"che 'l mio\" found in text\n",
      "\"'l mio duca\" found in text\n",
      "\"che 'n su\" found in text\n",
      "\"'n su lo\" found in text\n",
      "\"Cosi` prendemmo via\" found in text\n",
      "\"e 'l petto\" found in text\n",
      "\"che nel mondo\" found in text\n",
      "\"la testa e\" found in text\n",
      "\"Allor mi volsi\" found in text\n",
      "\"mi volsi a\" found in text\n",
      "\"E io li\" found in text\n",
      "\"disse: \"Chi fosti,\" found in text\n",
      "\"\"Chi fosti, che\" found in text\n",
      "\"fosti, che per\" found in text\n",
      "\"che per tante\" found in text\n",
      "\"per tante punte\" found in text\n",
      "\"a tai forte\" found in text\n",
      "\"Io venni i\" found in text\n",
      "\"Mentr' io la`\" found in text\n",
      "\"io la` giu`\" found in text\n",
      "\"fondo che di\" found in text\n",
      "\"su la terra\" found in text\n",
      "\"che de la\" found in text\n",
      "\"de la bolgia\" found in text\n",
      "\"di Malebolge e\" found in text\n",
      "\"Malebolge e li\" found in text\n",
      "\"disse a me:\" found in text\n",
      "\"a me: \"Elli\" found in text\n",
      "\"al pie` del\" found in text\n",
      "\"di qua, di\" found in text\n",
      "\"qua, di la`\" found in text\n",
      "\"son li spirti\" found in text\n",
      "\"che di tutte\" found in text\n",
      "\"a la gente\" found in text\n",
      "\"E quella che\" found in text\n",
      "\"quella che ricuopre\" found in text\n",
      "\"innanzi a la\" found in text\n",
      "\"a la riva,\" found in text\n",
      "\"e l'altro dietro\" found in text\n",
      "\"l'altro dietro a\" found in text\n",
      "\"dietro a lui\" found in text\n",
      "\"a lui a\" found in text\n",
      "\"mossi la voce:\" found in text\n",
      "\"la voce: \"O\" found in text\n",
      "\"\"O tu che\" found in text\n",
      "\"tu che la\" found in text\n",
      "108 examples found out of 940\n"
     ]
    }
   ],
   "source": [
    "with open('commedia.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "def find_combinations(test_output, n):\n",
    "    word_list = test_output.split()\n",
    "\n",
    "    found_counter = 0\n",
    "    for i in range(len(word_list)-n+1):\n",
    "        search = \" \".join(word_list[i:i+n])\n",
    "        if search in text:\n",
    "            print(f'\"{search}\" found in text')\n",
    "            found_counter += 1\n",
    "\n",
    "    print(f'{found_counter} examples found out of {len(word_list)-n+1}')\n",
    "find_combinations(test_output, 3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
