{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 128\n",
    "block_size = 32  # spatial extent of the model for its context\n",
    "max_iters = 5000  # number of training iterations\n",
    "eval_interval = 10  # frequency of printing training stats\n",
    "learning_rate = 3e-4\n",
    "device = torch.device('mps')\n",
    "eval_iters = 200  # number of iterations to evaluate the model\n",
    "n_embd = 600 \n",
    "n_layers = 6\n",
    "dropout = 0.2\n",
    "n_head = 6\n",
    "data_filepath = 'combined.txt'\n",
    "train_val_split = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Number of words: 2,622,431\n",
      "['murky', 'Replaced', 'Guidelines', 'Finite', 'Hullo', 'Bits', 'tie', 'resplendent', 'adept', 'Doxys', 'Fisheries', 'Father', 'sputtered', 'aquired', 'mammal', 'Snout', 'mounts', 'recipes', 'Slither', 'durable', 'damaged', 'delegations', 'leaves', 'crude', 'Than', 'tyke', 'Cries', 'cos', 'messages', 'staked', 'blizzard', 'fancies', 'service', 'treatment', 'Bugbear', 'advances', 'reserved', 'waltz', 'mornIng', 'Branstone', 'punishing', 'scrubs', 'tad', 'tense', 'Delighted', 'curly', 'amulets', 'flippers', 'springily', 'stricken']\n",
      " - Vocabulary size: 26,960\n",
      " - Length of train data: 2,491,309\n",
      " - Length of val data: 131,122\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize(text):\n",
    "    # Use regular expression to tokenize the text\n",
    "    tokens = re.findall(r'\\b\\w+\\b|\\s|\\S', text)\n",
    "    return tokens\n",
    "\n",
    "# Load data\n",
    "with open(data_filepath, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize into words\n",
    "words = tokenize(text) # list of words\n",
    "vocab = list(set(words))\n",
    "vocab_size = len(vocab)\n",
    "print(f' - Number of words: {len(words):,}')\n",
    "print(vocab[:50])\n",
    "print(f' - Vocabulary size: {vocab_size:,}')\n",
    "stoi = {word: i for i, word in enumerate(vocab)}  # string to int\n",
    "itos = {i: word for i, word in enumerate(vocab)}  # int to string\n",
    "\n",
    "\n",
    "def encode(s): return [stoi[word] for word in s]\n",
    "def decode(l): return ' '.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "# Split into train and validation\n",
    "data = torch.tensor(encode(words), dtype=torch.long)\n",
    "n = int(train_val_split * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f' - Length of train data: {len(train_data):,}')\n",
    "print(f' - Length of val data: {len(val_data):,}')\n",
    "\n",
    "\n",
    "def get_batch(split, batch_size, block_size):\n",
    "    \"\"\"\n",
    "    Get a batch of data for training or validation.\n",
    "\n",
    "    Parameters:\n",
    "    split (str): The split to get the data from. Can be 'train' or 'val'.\n",
    "    batch_size (int): The batch size.\n",
    "    block_size (int): The sequence length.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor, torch.Tensor: The input data (x) and target data (y) as tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module):\n",
    "    ''' A transformer language model. Parameters are defined in the hyperparameters section and do not need to be passed in.\n",
    "    The class has two methods: forward and generate. Forward is used for training and generate is used for sampling.\n",
    "    In the forward method, the input is a batch of sequences of tokens, and optionally a batch of target sequences of tokens.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_layers) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.final_ln = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        ''' idx is a batch of sequences of tokens. targets is a batch of target sequences of tokens. If targets is None, then the loss is not calculated.'''\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensors. where B is batch size and T is the number of tokens in each sequence (block_size*batch_size)\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.final_ln(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,V)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        ''' idx is a batch of sequences of tokens. max_new_tokens is the maximum number of tokens to generate.\n",
    "        returns both the whole sequence as well as the last generated token, for live generation.'''\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=-1)\n",
    "        return idx, idx_next\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''a single self attention head'''\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)  # (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B,T,C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        # Q. what is proj an abbreviation for? A. projection Q. What is meant by projection? A. I think it means that the output is the same size as the input.3\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)  # (B,T,C)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "\n",
    "        xmean = x.mean(1, keepdim=True)  # batch mean\n",
    "        xvar = x.var(1, keepdim=True)  # batch variance\n",
    "\n",
    "        # normalize to unit variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is running on mps\n",
      "Number of parameters: 58,355,360\n"
     ]
    }
   ],
   "source": [
    "m = Transformer().to(device)\n",
    "m.generate(torch.randint(vocab_size, (1, 1), device=device), 100)\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
    "lossi = []  # loss history\n",
    "current_step = 0\n",
    "print(f'Model is running on {device}')\n",
    "print(f'Number of parameters: {sum(p.numel() for p in m.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1280/5000|||average loss over last 10 steps: 2.45165|||time taken: 0:00:20|||estimated time remaining: 2:57:09 \n",
      "\n",
      "step: 1290/5000|||average loss over last 10 steps: 2.45472|||time taken: 0:00:42|||estimated time remaining: 2:32:46 \n",
      "\n",
      "step: 1300/5000|||average loss over last 10 steps: 2.43687|||time taken: 0:01:05|||estimated time remaining: 2:28:27 \n",
      "\n",
      "step: 1310/5000|||average loss over last 10 steps: 2.44756|||time taken: 0:01:33|||estimated time remaining: 2:34:35 \n",
      "\n",
      "step: 1320/5000|||average loss over last 10 steps: 2.44333|||time taken: 0:01:57|||estimated time remaining: 2:32:41 \n",
      "\n",
      "step: 1330/5000|||average loss over last 10 steps: 2.42834|||time taken: 0:02:17|||estimated time remaining: 2:27:01 \n",
      "\n",
      "step: 1340/5000|||average loss over last 10 steps: 2.44998|||time taken: 0:02:42|||estimated time remaining: 2:27:30 \n",
      "\n",
      "step: 1350/5000|||average loss over last 10 steps: 2.43788|||time taken: 0:03:09|||estimated time remaining: 2:29:19 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "current_step_this_run = 0\n",
    "while current_step < max_iters:\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "    # Print training stats\n",
    "    if current_step % eval_interval == 0 or current_step == max_iters - 1:\n",
    "        current_time = round(time.time() - start_time)\n",
    "        formatted_time = str(datetime.timedelta(seconds=round(current_time,2)))\n",
    "        \n",
    "        # Estimate remaining time\n",
    "        steps_remaining = max_iters - current_step\n",
    "        time_per_step = current_time / (current_step_this_run + 1)  # Avoid division by zero\n",
    "        remaining_time = datetime.timedelta(\n",
    "        \n",
    "            seconds=round(steps_remaining * time_per_step))\n",
    "        if current_step == 0:\n",
    "            average_loss = loss.item()\n",
    "        else:\n",
    "            average_loss = sum(lossi[-eval_interval:]) / eval_interval # This is not correct, it should be the average loss over the last eval_interval steps\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        print(f'step: {current_step}/{max_iters}|||average loss over last {eval_interval} steps: {round(average_loss,5)}|||time taken: {formatted_time}|||estimated time remaining: {remaining_time} \\n')\n",
    "    current_step += 1\n",
    "    current_step_this_run += 1\n",
    "\n",
    "# Print the total training time\n",
    "total_time = time.time() - start_time\n",
    "formatted_total_time = str(datetime.timedelta(seconds=total_time))\n",
    "print(f'Total training time: {formatted_total_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss history\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(np.log10(lossi))\n",
    "lossi_means = [sum(lossi[i:i+100])/100 for i in range(len(lossi)-100)]\n",
    "plt.plot(np.log10(lossi_means))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.2517848265171051\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop over validation set\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for steps in range(20):\n",
    "        xb, yb = get_batch('val', batch_size, block_size)\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits, loss = m(xb, yb)\n",
    "        total_loss += loss.item()\n",
    "    print(f'validation loss: {total_loss/eval_iters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " him spoke to with deal, but walked from his second and more gray inside. 'You come off here, which is our schools,' said Sirius warningly, watching you all over your getting powers and hear the kitchen at all, \"waved you Oh, Harry, too!\n",
      "\"mis\".\"\n",
      "\"Aguamenti --Ron - will come back in to touch Nearly Headless guests!\" Black shrieked angrily, ignoring but scooped on Harry with great bearable of scales. \"Elphias he owned me these at an excuse our broom, then I'stretched out of the Chamber when I fangs Hogwarts, so \n",
      "it crashed my wig might have old some-much creature. And or abroad they have been fifty hard at dinnertime - \n",
      "Considered score Christmas!” \n",
      "\n",
      "“Harry didn’t help most that unless Dumbledore don’t see Charlie!” said Ron excitedly, and Harry looked around at another wall and felt a dead of light on the wall as Mrs Figg at the hole wheeled along in week, Rookwood. He\n",
      "younger impossible to die, at least for Horcruxes. . . or Firenze kept having friends kept on it when he was injured, bathing watered of my trees earned living into my body\" and almost hissed but almost as though it seemed that marked abruptly. \"Have you charm fifty points off, oh preparation and will make it useful to be the Irish Art -' \n",
      "Harry opened his mouth so tightly realized that he would - what felt tell the instrument or Lord \n",
      "\n",
      " And sure that I will have a very much protection and outburst and a purple labelled banner at that moment looked indoors on as many ways to his pockets, then and continued to strike within this few sort of \n",
      "thousand fear and Magorian inside of them: The last pleased reverberated by "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(new_chars):\n\u001b[0;32m----> 6\u001b[0m         _,output \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m         decoded_output \u001b[38;5;241m=\u001b[39m decode([output\u001b[38;5;241m.\u001b[39mitem()])\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(decoded_output, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "Cell \u001b[0;32mIn[141], line 45\u001b[0m, in \u001b[0;36mTransformer.generate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# crop idx\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     idx_cond \u001b[38;5;241m=\u001b[39m idx[:, \u001b[38;5;241m-\u001b[39mblock_size:]\n\u001b[0;32m---> 45\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     47\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[141], line 25\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     22\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(\n\u001b[1;32m     23\u001b[0m     torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))  \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m(x)  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_ln(x)  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)  \u001b[38;5;66;03m# (B,T,V)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1682\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1683\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1684\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate from the model, printing the output as it goes\n",
    "new_chars = 10000\n",
    "idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "with torch.no_grad():\n",
    "    for _ in range(new_chars):\n",
    "        _,output = m.generate(idx, 1)\n",
    "        decoded_output = decode([output.item()])\n",
    "        print(decoded_output, end = '') \n",
    "        idx = torch.cat((idx, output), dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      4\u001b[0m decoded_output \u001b[38;5;241m=\u001b[39m decode(output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoded_output)\n",
      "Cell \u001b[0;32mIn[93], line 45\u001b[0m, in \u001b[0;36mTransformer.generate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# crop idx\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     idx_cond \u001b[38;5;241m=\u001b[39m idx[:, \u001b[38;5;241m-\u001b[39mblock_size:]\n\u001b[0;32m---> 45\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     47\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[93], line 25\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     22\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(\n\u001b[1;32m     23\u001b[0m     torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))  \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_ln(x)  \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)  \u001b[38;5;66;03m# (B,T,V)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[93], line 118\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    117\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x))\n\u001b[0;32m--> 118\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[93], line 104\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/general_ml/lib/python3.11/site-packages/torch/nn/functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "output = m.generate(idx, 200)[0].tolist()\n",
    "\n",
    "decoded_output = decode(output[0])\n",
    "print(decoded_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
